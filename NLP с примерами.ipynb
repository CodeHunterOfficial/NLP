{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Представление текста в цифровом виде для нейросети"
      ],
      "metadata": {
        "id": "BvzWrlwBR2If"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для того, чтобы использовать нейросеть для анализа текстов, необходимо представить текст в цифровом виде. Существует несколько подходов к представлению текста в цифровом виде, вот некоторые из них:\n",
        "\n",
        "1. Мешок слов (Bag of Words):\n",
        "\n",
        "* Создается словарь всех уникальных слов в корпусе текстов.\n",
        "* Каждому слову присваивается уникальный индекс в словаре.\n",
        "* Каждый текст представляется в виде вектора, где каждый элемент соответствует количеству вхождений соответствующего слова в тексте.\n",
        "* Этот подход учитывает только количество вхождений слов в текст, но не учитывает порядок слов.\n",
        "\n",
        "2. TF-IDF (Term Frequency - Inverse Document Frequency):\n",
        "\n",
        "* Создается словарь всех уникальных слов в корпусе текстов.\n",
        "* Каждому слову присваивается уникальный индекс в словаре.\n",
        "* Каждый текст представляется в виде вектора, где каждый элемент соответствует значению TF-IDF для соответствующего слова в тексте.\n",
        "* Значение TF-IDF для каждого слова вычисляется как произведение частоты вхождения слова в текст (Term Frequency) и обратной частоты вхождения слова в корпус текстов (Inverse Document Frequency).\n",
        "* Этот подход учитывает не только количество вхождений слов в текст, но и их важность для всего корпуса текстов.\n",
        "\n",
        "3. Word Embeddings:\n",
        "\n",
        "* Создается модель нейросети, которая обучается на задаче предсказания следующего слова в тексте.\n",
        "* В результате обучения модели каждому слову присваивается вектор фиксированной длины.\n",
        "* Этот подход учитывает не только количество вхождений слов в текст и их важность для всего корпуса текстов, но и семантический контекст, в котором используется каждое слово.\n",
        "* В зависимости от задачи и доступных ресурсов выбирается подход к представлению текста в цифровом виде."
      ],
      "metadata": {
        "id": "_vitVBUJR5Cs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Приведем конкретные примеры\n",
        "\n",
        "\n",
        "1. Мешок слов (Bag of Words):\n",
        "\n",
        "* Исходный текст: \"Коты любят есть рыбу, а собаки - мясо.\"\n",
        "* Словарь: {\"коты\": 0, \"любят\": 1, \"есть\": 2, \"рыбу\": 3, \"а\": 4, \"собаки\": 5, \"мясо\": 6}\n",
        "* Представление текста: [1, 1, 1, 1, 1, 0, 0]\n",
        "\n",
        "2. TF-IDF (Term Frequency - Inverse Document Frequency):\n",
        "\n",
        "* Исходный текст: \"Коты любят есть рыбу, а собаки - мясо.\"\n",
        "* Словарь: {\"коты\": 0, \"любят\": 1, \"есть\": 2, \"рыбу\": 3, \"а\": 4, \"собаки\": 5, \"мясо\": 6}\n",
        "* Представление текста: [0.4, 0.4, 0.4, 0.4, 0.4, 0, 0]\n",
        "\n",
        "3. Word Embeddings:\n",
        "\n",
        "* Исходный текст: \"Коты любят есть рыбу, а собаки - мясо.\"\n",
        "* Представление текста: Вектор размерности, например, 100, содержащий числа, которые представляют семантический контекст каждого слова в тексте. Например, [0.2, 0.3, -0.1, ...] для слова \"коты\"."
      ],
      "metadata": {
        "id": "jsbG2pDjSmEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ПРиведем примеры того, как можно реализовать представления текста в цифровом виде для каждого из методов на Python в среде Colab:\n",
        "\n",
        "1. Мешок слов (Bag of Words):"
      ],
      "metadata": {
        "id": "azKlnPpHS_a7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Создаем список текстов\n",
        "texts = [\"Коты любят есть рыбу, а собаки - мясо.\", \"Рыба - это любимая еда котов.\"]\n",
        "\n",
        "# Создаем объект CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Преобразуем тексты в матрицу с помощью метода fit_transform\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Выводим словарь всех уникальных слов\n",
        "print(vectorizer.get_feature_names())\n",
        "\n",
        "# Выводим представление текстов в виде векторов\n",
        "print(X.toarray())\n"
      ],
      "metadata": {
        "id": "3B6b5NY0TETT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. TF-IDF (Term Frequency - Inverse Document Frequency):"
      ],
      "metadata": {
        "id": "farYPIqETJ2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Создаем список текстов\n",
        "texts = [\"Коты любят есть рыбу, а собаки - мясо.\", \"Рыба - это любимая еда котов.\"]\n",
        "\n",
        "# Создаем объект TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Преобразуем тексты в матрицу с помощью метода fit_transform\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Выводим словарь всех уникальных слов\n",
        "print(vectorizer.get_feature_names())\n",
        "\n",
        "# Выводим представление текстов в виде векторов\n",
        "print(X.toarray())\n"
      ],
      "metadata": {
        "id": "yelskCNWTNwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Word Embeddings:"
      ],
      "metadata": {
        "id": "BjNR47duTTHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Создаем список текстов\n",
        "texts = [\"Коты любят есть рыбу, а собаки - мясо.\", \"Рыба - это любимая еда котов.\"]\n",
        "\n",
        "# Создаем объект Tokenizer и обучаем его на текстах\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Преобразуем тексты в последовательности индексов слов\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Добавляем заполнение до фиксированной длины последовательностей\n",
        "padded_sequences = pad_sequences(sequences, maxlen=10, padding=\"post\")"
      ],
      "metadata": {
        "id": "m1WrpApTTUXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем модель нейросети и обучаем ее на последовательностях\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(units=1, activation=\"sigmoid\")\n",
        "])"
      ],
      "metadata": {
        "id": "X6-qJlgaTbja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Этот код на языке Python создает модель нейронной сети, используя Keras API из библиотеки TensorFlow. Модель состоит из трех последовательно связанных слоев:\n",
        "\n",
        "* Слой Embedding, который преобразует входные данные в векторы фиксированной длины. В данном случае, используется размерность векторов равная 100.\n",
        "* Слой GlobalAveragePooling1D, который усредняет значения признаков по всей длине входных данных и сжимает их до одного значения.\n",
        "* Полносвязный слой Dense с одним выходным нейроном и функцией активации sigmoid, который выполняет бинарную классификацию.\n",
        "\n",
        "Кроме того, в первом слое Embedding используется параметр input_dim, который равен длине словаря (tokenizer.word_index) плюс один, так как словарь начинается с индекса 1, а не 0. Это позволяет создать векторы для всех слов в словаре, включая неизвестные слова."
      ],
      "metadata": {
        "id": "P96sgvlOTkWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.fit(padded_sequences, [0, 1], epochs=10)\n",
        "\n",
        "# Получаем векторное представление слов\n",
        "word_embeddings = model.layers[0].get_weights()[0]\n",
        "print(word_embeddings)"
      ],
      "metadata": {
        "id": "Tg5fKmtITd_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Анализ тональности отзывов на фильмы IMDB | Нейросети для анализа текстов"
      ],
      "metadata": {
        "id": "Ts_h79L-T2eE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для анализа тональности отзывов на фильмы IMDB можно использовать нейросети. Один из подходов - использование рекуррентных нейронных сетей (RNN) для анализа текстов. Вот пример кода на Python, который использует библиотеку Keras для создания и обучения RNN для анализа тональности отзывов на фильмы IMDB:"
      ],
      "metadata": {
        "id": "iDqBhrYiJyRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Импортируем необходимые библиотеки\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "import tensorflow as tf\n",
        "\n",
        "# Загружаем набор данных IMDB\n",
        "max_features = 20000\n",
        "maxlen = 80\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6Bn5w5jJ8k3",
        "outputId": "0966042e-2547-480f-b68c-f98ea74a1bcd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ограничиваем длину отзывов до maxlen слов\n",
        "print('Pad sequences (samples x time)')\n",
        "\n",
        "input_train=tf.keras.preprocessing.sequence.pad_sequences(input_train, maxlen=maxlen)\n",
        "\n",
        "input_test=tf.keras.preprocessing.sequence.pad_sequences(input_test, maxlen=maxlen)\n",
        "\n",
        "#input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
        "#input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AGjG64hL_S8",
        "outputId": "7e8b4153-2c14-4a69-fd77-7c9f52d036d4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pad sequences (samples x time)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем модель RNN\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "Os4sAlJBOPwm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Эта нейронная сеть содержит слои Embedding, LSTM и Dense. \n",
        "\n",
        "* Embedding-слой используется для преобразования входных данных в векторы фиксированной длины, \n",
        "\n",
        "* LSTM-слой используется для обработки последовательных данных, \n",
        "\n",
        "* Dense-слой используется для выхода одного значения с функцией активации sigmoid."
      ],
      "metadata": {
        "id": "ZW__NooSO83I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Компилируем модель\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "6AbIs6yLO7eC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучаем модель на тренировочных данных\n",
        "print('Training...')\n",
        "model.fit(input_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=15,\n",
        "          validation_data=(input_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fn58ChZROQ_1",
        "outputId": "aab84b8f-1f7d-40d2-bca3-f98ed0b4fde2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n",
            "Epoch 1/15\n",
            "782/782 [==============================] - 285s 360ms/step - loss: 0.4236 - accuracy: 0.8038 - val_loss: 0.3540 - val_accuracy: 0.8438\n",
            "Epoch 2/15\n",
            "782/782 [==============================] - 263s 336ms/step - loss: 0.2567 - accuracy: 0.8978 - val_loss: 0.3793 - val_accuracy: 0.8286\n",
            "Epoch 3/15\n",
            "782/782 [==============================] - 277s 355ms/step - loss: 0.1616 - accuracy: 0.9392 - val_loss: 0.4821 - val_accuracy: 0.8277\n",
            "Epoch 4/15\n",
            "782/782 [==============================] - 292s 373ms/step - loss: 0.1109 - accuracy: 0.9598 - val_loss: 0.5485 - val_accuracy: 0.8194\n",
            "Epoch 5/15\n",
            "782/782 [==============================] - 327s 418ms/step - loss: 0.0766 - accuracy: 0.9730 - val_loss: 0.6402 - val_accuracy: 0.8234\n",
            "Epoch 6/15\n",
            "782/782 [==============================] - 305s 391ms/step - loss: 0.0523 - accuracy: 0.9830 - val_loss: 0.7606 - val_accuracy: 0.8196\n",
            "Epoch 7/15\n",
            "782/782 [==============================] - 286s 366ms/step - loss: 0.0459 - accuracy: 0.9837 - val_loss: 0.7614 - val_accuracy: 0.8182\n",
            "Epoch 8/15\n",
            "782/782 [==============================] - 304s 389ms/step - loss: 0.0289 - accuracy: 0.9915 - val_loss: 0.7614 - val_accuracy: 0.8113\n",
            "Epoch 9/15\n",
            "782/782 [==============================] - 356s 455ms/step - loss: 0.0215 - accuracy: 0.9935 - val_loss: 1.0012 - val_accuracy: 0.8104\n",
            "Epoch 10/15\n",
            "512/782 [==================>...........] - ETA: 1:46 - loss: 0.0190 - accuracy: 0.9946"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Этот код  использует метод fit() для обучения нейронной сети. Метод fit() принимает на вход обучающие данные (input_train) и соответствующие им метки (y_train), а также параметры обучения, такие как размер пакета (batch_size) и количество эпох (epochs). Также, в данном случае, используется валидационный набор данных, который передается в параметре validation_data. Метод fit() обучает модель на обучающих данных и вычисляет значение функции потерь на каждой эпохе, а также метрики качества модели. В конце обучения, метод fit() возвращает историю обучения, которая содержит значения функции потерь и метрик на каждой эпохе."
      ],
      "metadata": {
        "id": "IpyRrjx8Oq27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Оцениваем точность модели на тестовых данных\n",
        "print('Testing...')\n",
        "score, acc = model.evaluate(input_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "metadata": {
        "id": "mSq2wcUAOSUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Этот код загружает набор данных IMDB, ограничивает длину отзывов до maxlen слов, создает RNN модель с использованием слоев Embedding и LSTM, компилирует модель и обучает ее на тренировочных данных. Затем модель оценивается на тестовых данных, и выводится точность модели на тестовых данных."
      ],
      "metadata": {
        "id": "4HXhFh23KB3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Представление текста вектором One Hot Encoding | Нейросети для анализа текстов"
      ],
      "metadata": {
        "id": "lPwm0gnWUMvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One Hot Encoding - это метод представления категориальных данных в виде векторов, в котором каждый элемент вектора соответствует одному из возможных значений категории. В текстовом контексте, каждый элемент может соответствовать отдельному слову в словаре. Таким образом, каждый документ представляется в виде вектора, где каждый элемент соответствует наличию или отсутствию соответствующего слова в тексте.\n",
        "\n",
        "Нейросети для анализа текстов используются для автоматической обработки и классификации текстовых данных. Они могут быть использованы для анализа тональности отзывов, определения тематики статей и многого другого."
      ],
      "metadata": {
        "id": "5b5zWfcAUOm8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Конкретный пример One Hot Encoding для текста может выглядеть следующим образом:\n",
        "\n",
        "Предположим, у нас есть словарь из 5 слов: \"яблоко\", \"банан\", \"апельсин\", \"груша\" и \"ананас\". Текст \"Я люблю есть яблоки и бананы\" можно представить в виде вектора One Hot Encoding следующим образом: [1, 1, 0, 0, 0], где первый элемент соответствует слову \"яблоко\", второй элемент - слову \"банан\", а оставшиеся элементы - другим словам из словаря.\n",
        "\n",
        "Примеры использования нейросетей для анализа текстов:\n",
        "\n",
        "* Классификация текстов на позитивные и негативные отзывы о продукте или услуге\n",
        "* Определение тематики статьи или сообщения в социальных сетях\n",
        "* Автоматический перевод текста на другой язык\n",
        "* Генерация текста на основе заданных параметров"
      ],
      "metadata": {
        "id": "KG1xukTBUVjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Конкретный пример One Hot Encoding для текста на языке Python (в Google Colab) может выглядеть следующим образом:"
      ],
      "metadata": {
        "id": "urAcJbIZUyV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Создаем пример текста\n",
        "text = [\"Я люблю есть яблоки и бананы\"]\n",
        "\n",
        "# Создаем экземпляр OneHotEncoder и обучаем его на тексте\n",
        "encoder = OneHotEncoder()\n",
        "encoder.fit(text)\n",
        "\n",
        "# Преобразуем текст в вектор One Hot Encoding\n",
        "vector = encoder.transform(text).toarray()\n",
        "\n",
        "print(vector)\n"
      ],
      "metadata": {
        "id": "duvKZpasU07y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результатом будет вектор One Hot Encoding, соответствующий тексту \"Я люблю есть яблоки и бананы\"."
      ],
      "metadata": {
        "id": "E4AjZGFKU7hc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример использования нейросетей для анализа текстов на языке Python (в Google Colab) может выглядеть следующим образом:"
      ],
      "metadata": {
        "id": "F7AiTu_iU9lN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Создаем пример текста и меток классов\n",
        "texts = [\"Это очень хороший продукт\", \"Этот продукт ужасен\"]\n",
        "labels = [1, 0]\n",
        "\n",
        "# Создаем экземпляр Tokenizer и обучаем его на тексте\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Преобразуем текст в числовые последовательности\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Создаем модель нейросети\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=len(tokenizer.word_index), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Компилируем модель\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
        "\n",
        "# Обучаем модель на данных\n",
        "model.fit(sequences, labels, epochs=10, batch_size=1)\n",
        "\n",
        "# Прогнозируем метки классов для новых текстов\n",
        "new_texts = [\"Я люблю этот продукт\", \"Этот продукт не очень хорош\"]\n",
        "new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
        "predictions = model.predict_classes(new_sequences)\n",
        "\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "id": "EuG1XCOdU-Gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результатом будет прогноз меток классов для новых текстов. В данном случае, нейросеть будет классифицировать тексты на позитивные и негативные."
      ],
      "metadata": {
        "id": "MzVanqX-VE-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Представление текста плотным вектором"
      ],
      "metadata": {
        "id": "vKaL0Jn5VZOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Представление текста плотным вектором (Dense Vector Representation) - это метод представления текстовой информации в виде вектора чисел фиксированной длины. В отличие от метода One Hot Encoding, где каждое слово представляется отдельным бинарным признаком, при представлении текста плотным вектором каждое слово представляется в виде вектора небольшой размерности, обычно от 50 до 300.\n",
        "\n",
        "Для получения плотного векторного представления текста используются различные алгоритмы, такие как Word2Vec, GloVe и FastText. Эти алгоритмы обучаются на больших корпусах текстов и позволяют получить векторные представления слов, которые учитывают контекст, в котором слово употребляется.\n",
        "\n",
        "Пример использования нейросетей для анализа текстов с использованием плотных векторных представлений может выглядеть следующим образом:"
      ],
      "metadata": {
        "id": "mqKo8yNJVodF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, Flatten\n",
        "\n",
        "# Создаем пример текста и меток классов\n",
        "texts = [\"Это очень хороший продукт\", \"Этот продукт ужасен\"]\n",
        "labels = [1, 0]\n",
        "\n",
        "# Создаем экземпляр Tokenizer и обучаем его на тексте\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Преобразуем текст в числовые последовательности\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Добавляем паддинг до максимальной длины последовательности\n",
        "maxlen = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=maxlen, padding='post')\n",
        "\n",
        "# Загружаем предобученные векторные представления слов\n",
        "embeddings_index = {}\n",
        "with open('path/to/embeddings_file.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "# Создаем матрицу весов для слоя Embedding\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Создаем модель нейросети\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(tokenizer.word_index) + 1,\n",
        "                    embedding_dim,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=maxlen,\n",
        "                    trainable=False))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Компилируем модель\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
        "\n",
        "# Обучаем модель на данных\n",
        "model.fit(padded_sequences, labels, epochs=10, batch_size=1)\n",
        "\n",
        "# Прогнозируем метки классов для новых текстов\n",
        "new_texts = [\"Я люблю этот продукт\", \"Этот продукт не очень хорош\"]\n",
        "new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
        "new_padded_sequences = pad_sequences(new_sequences, maxlen=maxlen, padding='post')\n",
        "predictions = model.predict_classes(new_padded_sequences)\n",
        "\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "aRp34GJ4WSBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В данном примере мы использовали предобученные векторные представления слов для создания матрицы весов слоя Embedding. Затем мы создали модель нейросети, которая состоит из слоя Embedding, слоя Flatten, двух полносвязных слоев и слоя активации sigmoid. Обучили модель на данных и прогнозировали метки классов для новых"
      ],
      "metadata": {
        "id": "7zYtmgPTW3Pk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Плотное векторное представление слов для определения тональности текста отзывов на фильмы из IMDb (Internet Movie Database) \n",
        "\n",
        "\n",
        "Чтобы запускать и редактировать код, сохраните копию этого ноутбука себе (File->Save a copy in Drive...). Свою копию вы сможете изменять и запускать.\n",
        "\n",
        "Не забудьте подключить GPU, чтобы сеть обучалась быстрее (Runtime -> Change Runtime Type -> Hardware Accelerator -> GPU)."
      ],
      "metadata": {
        "id": "0k73_k-RXFsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Набор данных IMDb movie review\n",
        "\n",
        "[Набор данных IMDb movie review](https://ai.stanford.edu/~amaas/data/sentiment/) создан для задач определения тональности текста. Набор включает отзывы на фильмы с сайта [IMDb](https://www.imdb.com). Отзывы только явно положительные (оценка >= 7) или отрицательные (оценка <= 4), нейтральные отзывы в набор данных не включались.\n",
        "\n",
        "Размер набора данных 50 тыс. отзывов:\n",
        "- Набор данных для обучения - 25 тыс. отзывов\n",
        "- Набор данных для тестирования - 25 тыс. отзывов\n",
        "\n",
        "Количество положительных и отрицательных отзывов одинаковое.\n",
        "\n",
        "Разметка набора данных:\n",
        "- 0 - отзыв отрицательный\n",
        "- 1 - отзыв положительный\n",
        "\n",
        "С точки зрения машинного обучения это задача бинарной классификации.\n",
        "\n",
        "Набор данных описан в статье: [Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)](https://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf)."
      ],
      "metadata": {
        "id": "WHEpRIBtXMte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www.dropbox.com/s/grd17bkapocb92o/imdb_movie_reviews.png?dl=1\" width=\"600\">\n"
      ],
      "metadata": {
        "id": "GtMtltabXQH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten, Dropout\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "%matplotlib inline "
      ],
      "metadata": {
        "id": "oujZTMoYXVJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Загружаем данные"
      ],
      "metadata": {
        "id": "ByMk00BeXZfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_words=10000"
      ],
      "metadata": {
        "id": "E4B2bnggXcG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)"
      ],
      "metadata": {
        "id": "ylPCKvf7XeTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Просмотр данных\n",
        "**Рецензия**"
      ],
      "metadata": {
        "id": "QDb1y8HEXhCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[3]"
      ],
      "metadata": {
        "id": "YgFx8NEtXl9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Правильный ответ"
      ],
      "metadata": {
        "id": "bOSxs_8pXtMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[3]"
      ],
      "metadata": {
        "id": "HMx8QM46XwPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Подготовка данных для обучения"
      ],
      "metadata": {
        "id": "ib26KFzbXz-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 200"
      ],
      "metadata": {
        "id": "YWQJ_9alX2Xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = pad_sequences(x_train, maxlen=maxlen, padding='post')\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen, padding='post')"
      ],
      "metadata": {
        "id": "4uzF73OLXycN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[1]"
      ],
      "metadata": {
        "id": "7pSiwWSIX6wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[1]"
      ],
      "metadata": {
        "id": "IlDRionRX8zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Создание нейронной сети"
      ],
      "metadata": {
        "id": "jNQ96AmGX-zP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 2, input_length=maxlen))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "xAAe53d2X_SB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', \n",
        "              loss='binary_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "31ylAcVgYBYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Обучаем нейронную сеть"
      ],
      "metadata": {
        "id": "eJ7flP3gYFDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, \n",
        "                    y_train, \n",
        "                    epochs=15,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.1)"
      ],
      "metadata": {
        "id": "xyA-6EqlYHR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['acc'], \n",
        "         label='Доля верных ответов на обучающем наборе')\n",
        "plt.plot(history.history['val_acc'], \n",
        "         label='Доля верных ответов на проверочном наборе')\n",
        "plt.xlabel('Эпоха обучения')\n",
        "plt.ylabel('Доля верных ответов')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "58xrb7W1YHu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Проверяем работу сети на тестовом наборе данных"
      ],
      "metadata": {
        "id": "h9iuqlvcYOxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(x_test, y_test, verbose=1)"
      ],
      "metadata": {
        "id": "sRLMG-XbYQiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Исследуем обученное плотное векторное представление слов\n",
        "\n",
        "**Получаем матрицу плотных векторных представлений слов**"
      ],
      "metadata": {
        "id": "7i26FOEJYSqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = model.layers[0].get_weights()[0]"
      ],
      "metadata": {
        "id": "iRZoxjbfYWzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix[:5]"
      ],
      "metadata": {
        "id": "u-LQkE89YY8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Загружаем словарь с номерами слов**"
      ],
      "metadata": {
        "id": "zMtdKf2rYcy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index_org = imdb.get_word_index()"
      ],
      "metadata": {
        "id": "K_domlzjYbzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Дополняем словарь служебными символами\n",
        "word_index = dict()\n",
        "for word,number in word_index_org.items():\n",
        "    word_index[word] = number + 3\n",
        "word_index[\"<Заполнитель>\"] = 0\n",
        "word_index[\"<Начало последовательности>\"] = 1\n",
        "word_index[\"<Неизвестное слово>\"] = 2  \n",
        "word_index[\"<Не используется>\"] = 3"
      ],
      "metadata": {
        "id": "Du8KxXkOYkJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ищем векторы для слов**"
      ],
      "metadata": {
        "id": "2_pRobCMYrBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'good'\n",
        "word_number = word_index[word]\n",
        "print('Номер слова', word_number)\n",
        "print('Вектор для слова', embedding_matrix[word_number])"
      ],
      "metadata": {
        "id": "ap_bo20yYrhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Сохраняем обученные плотные векторные представления в файл\n",
        "\n",
        "**Составляем реверсивный словарь токенов (слов)**"
      ],
      "metadata": {
        "id": "t_zCNhGQYvae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_word_index = dict()\n",
        "for key, value in word_index.items():\n",
        "    reverse_word_index[value] = key"
      ],
      "metadata": {
        "id": "tvfQ4rSWY1Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Записываем плотные векторные представления в файл**"
      ],
      "metadata": {
        "id": "0Td46ZIfYwFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'imdb_embeddings.csv'\n",
        "\n",
        "with open(filename, 'w') as f:\n",
        "    for word_num in range(max_words):\n",
        "      word = reverse_word_index[word_num]\n",
        "      vec = embedding_matrix[word_num]\n",
        "      f.write(word + \",\")\n",
        "      f.write(','.join([str(x) for x in vec]) + \"\\n\")"
      ],
      "metadata": {
        "id": "ReAydHmvY6Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 20 $filename"
      ],
      "metadata": {
        "id": "EpDMACrtY9aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Сохраняем файл на локальный компьютер**"
      ],
      "metadata": {
        "id": "yGprL1BHY_UJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('imdb_embeddings.csv')"
      ],
      "metadata": {
        "id": "dmmgx10MZBFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Визуализация плотных векторных представлений слов"
      ],
      "metadata": {
        "id": "YpDeOSBhZC53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(embedding_matrix[:,0], embedding_matrix[:,1])"
      ],
      "metadata": {
        "id": "3Y14aBwNZDXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выбираем коды слов, по которым можно определить тональность отзыва"
      ],
      "metadata": {
        "id": "1hUDIlGJZHKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review = ['brilliant', 'fantastic', 'amazing', 'good',\n",
        "          'bad', 'awful','crap', 'terrible', 'trash']\n",
        "enc_review = []\n",
        "for word in review:\n",
        "    enc_review.append(word_index[word])\n",
        "enc_review"
      ],
      "metadata": {
        "id": "q2IQRSgTZJNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получаем векторное представление интересующих нас слов"
      ],
      "metadata": {
        "id": "RcuKFcgiZLUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_vectors = embedding_matrix[enc_review]\n",
        "review_vectors"
      ],
      "metadata": {
        "id": "U3xHK8riZNQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Визуализация обученного плотного векторного представления слов, по которым можно определить эмоциональную окраску текста"
      ],
      "metadata": {
        "id": "5ESCbhrcZPAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(review_vectors[:,0], review_vectors[:,1])\n",
        "for i, txt in enumerate(review):\n",
        "    plt.annotate(txt, (review_vectors[i,0], review_vectors[i,1]))"
      ],
      "metadata": {
        "id": "dilwfT4eZRHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Рекуррентные нейросети"
      ],
      "metadata": {
        "id": "8JHHVxYuXDqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Литуратура\n",
        "\n",
        "https://github.com/fchollet/deep-learning-with-python-notebooks/tree/master"
      ],
      "metadata": {
        "id": "2Thjt7S-Jz6f"
      }
    }
  ]
}