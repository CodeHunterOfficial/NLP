{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMP4M4nRA66SkrwVQwx2kVq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/NLP_Spacy_Python/blob/main/Chapter2-5_Text_processing_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JApn4dGjtii"
      },
      "outputs": [],
      "source": [
        "# https://spacy.io/usage/models\n",
        "#page 43\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc=nlp(u'I am flying to Frisco')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([w.text for w in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7B6JvUeWlx03",
        "outputId": "89185f8c-b07a-4bbc-e9dd-c49e2ae978bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'flying', 'to', 'Frisco']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#page 44 Лематизация\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc=nlp(u'This product integrates both libraries for downloading and applying patches')\n",
        "for token in doc:\n",
        "  print(token.text, token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWE-96nSkfAf",
        "outputId": "bf8159ef-9687-4a56-af22-d939dab69a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This this\n",
            "product product\n",
            "integrates integrate\n",
            "both both\n",
            "libraries library\n",
            "for for\n",
            "downloading download\n",
            "and and\n",
            "applying apply\n",
            "patches patch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#46-47 pages\n",
        "import spacy\n",
        "from spacy.symbols import ORTH, LEMMA\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp('I am flying to San Francisco')\n",
        "print([w.text for w in doc])\n",
        "\n",
        "special_case = [{'ORTH': \"San \"}, {'ORTH': \"Francisco\"}]\n",
        "\n",
        "nlp.tokenizer.add_special_case(\"San Francisco\", special_case)\n",
        "\n",
        "print([w.lemma_ for w in nlp(u'I am flying to San Francisco')])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1D0jITblusz8",
        "outputId": "2719714c-d5d5-4eb9-b78c-98b36e57e8b0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'flying', 'to', 'San', 'Francisco']\n",
            "['I', 'be', 'fly', 'to', 'San ', 'Francisco']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(\"wanna go get coffee?\")\n",
        "print([w.text for w in doc])\n",
        "\n",
        "special_case = [{'ORTH': \"wan\"}, {'ORTH': \"na\"}]\n",
        "\n",
        "nlp.tokenizer.add_special_case(\"wanna\", special_case)\n",
        "\n",
        "print([w.text for w in nlp(\"wanna go get coffee?\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUU9ul51EFjK",
        "outputId": "581947e8-7f8d-4455-d223-6418857c0b73"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wanna', 'go', 'get', 'coffee', '?']\n",
            "['wan', 'na', 'go', 'get', 'coffee', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#50 page\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm') \n",
        "doc = nlp(u'I have flown to LA. Now I am flying to Frisco.') \n",
        "print([w.text for w in doc if w.tag_== 'VBG' or w.tag_== 'VB'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyHV3jN-vTfY",
        "outputId": "60f5c97e-e2e6-45dd-92b8-6da6d401a774"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['flying']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#51 page\n",
        "print([w.text for w in doc if w.pos_ == 'PROPN'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8ffsfGiEZw6",
        "outputId": "14d67365-e210-4777-f6e5-53c6350ae20c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['LA', 'Frisco']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy \n",
        "nlp = spacy.load('en_core_web_sm') \n",
        "doc = nlp(u'I have flown to LA. Now I am flying to Frisco.')\n",
        "for token in doc:  \n",
        "  print(token.text, token.pos_, token.dep_)"
      ],
      "metadata": {
        "id": "hiO4dQMnuagL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85a1bccf-125c-49dc-fd5c-9a5f8f9b224a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I PRON nsubj\n",
            "have AUX aux\n",
            "flown VERB ROOT\n",
            "to ADP prep\n",
            "LA PROPN pobj\n",
            ". PUNCT punct\n",
            "Now ADV advmod\n",
            "I PRON nsubj\n",
            "am AUX aux\n",
            "flying VERB ROOT\n",
            "to ADP prep\n",
            "Frisco PROPN pobj\n",
            ". PUNCT punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:  \n",
        "  print(token.head.text, token.dep_, token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7cdvOZIE3S4",
        "outputId": "5f0d3b8e-8490-4bbb-8ea4-40abe46d4db2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flown nsubj I\n",
            "flown aux have\n",
            "flown ROOT flown\n",
            "flown prep to\n",
            "to pobj LA\n",
            "flown punct .\n",
            "flying advmod Now\n",
            "flying nsubj I\n",
            "flying aux am\n",
            "flying ROOT flying\n",
            "flying prep to\n",
            "to pobj Frisco\n",
            "flying punct .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#56 page\n",
        "import spacy \n",
        "nlp = spacy.load('en_core_web_sm') \n",
        "doc = nlp(u'I have flown to LA. Now I am flying to Frisco.') \n",
        "for sent in doc.sents:  \n",
        "  print([w.text for w in sent if w.dep_ == 'ROOT' or w.dep_ == 'pobj'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsRFL5eTE7O0",
        "outputId": "806ae045-2440-4b4d-993f-7f036193eb6d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['flown', 'LA']\n",
            "['flying', 'Frisco']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#58 page\n",
        "import spacy \n",
        "nlp = spacy.load('en_core_web_sm') \n",
        "doc = nlp(u'I have flown to LA. Now I am flying to Frisco.') \n",
        "for token in doc:  \n",
        "  if token.ent_type != 0:    \n",
        "    print(token.text, token.ent_type_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oE8H7NAFN4E",
        "outputId": "fb8b4165-c086-4155-dbab-14ff9a270891"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LA GPE\n",
            "Frisco ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#61 page\n",
        "import spacy \n",
        "nlp = spacy.load('en_core_web_sm') \n",
        "doc = nlp(u'A severe storm hit the beach. ') \n",
        "print([doc[i] for i in range(len(doc))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuU5rS3LFkJ8",
        "outputId": "9533c837-5a95-46d9-9b37-5c9108ba21ae"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[A, severe, storm, hit, the, beach, .]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#61 page\n",
        "from spacy.tokens.doc import Doc \n",
        "from spacy.vocab import Vocab \n",
        "doc = Doc(Vocab(), words=[u'Hi', u'there'])\n",
        "print(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRM9zqiFFny_",
        "outputId": "6a7d2bb7-7ab5-4e86-ec31-69781df28e45"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi there \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 62 page\n",
        "import spacy \n",
        "doc = nlp(u'I want a green apple.')\n",
        "print([w for w in doc[4].lefts])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sh-5uJCNGYCQ",
        "outputId": "5c3964ed-430c-4c4e-9cc4-f2cfbc9d8e53"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[a, green]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#63 page\n",
        "import spacy \n",
        "doc = nlp(u'I want a green apple.')\n",
        "print([w for w in doc[4].children])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4vU4rYiGrbz",
        "outputId": "2af66e5f-8f23-45a1-8f6e-8d6b169f4535"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[a, green]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#63 page\n",
        "doc = nlp(u'A severe storm hit the beach. It started to rain.') \n",
        "for sent in doc.sents: \n",
        "  print([sent[i] for i in range(len(sent))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWzQpukWG8Fe",
        "outputId": "3a564fb2-ce00-43bd-a9c8-02ddc273c759"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[A, severe, storm, hit, the, beach, .]\n",
            "[It, started, to, rain, .]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#64 page \n",
        "[doc[i] for i in range(len(doc))]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7ofeFZ3Jjhb",
        "outputId": "ecb8b3f3-9169-4ce3-deb1-3c167bd5a650"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[A, severe, storm, hit, the, beach, ., It, started, to, rain, .]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 64 page\n",
        "for i,sent in enumerate(doc.sents): \n",
        "  if i==1 and sent[0].pos_== 'PRON': \n",
        "    print('The second sentence begins with a pronoun.') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-B-XKejqJxv1",
        "outputId": "e5f55023-28bd-42c6-9791-eeb0aaf38edc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The second sentence begins with a pronoun.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#64 page\n",
        "counter = 0 \n",
        "for sent in doc.sents: \n",
        "  if sent[len(sent)-2].pos_ == 'VERB':\n",
        "    counter+=1\n",
        "print(counter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EVzk-IUJ6cb",
        "outputId": "ea1fb605-a3e7-4625-f0a0-72e83acf959b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#64 page\n",
        "doc = nlp(u'A noun chunk is a phrase that has a noun as its head.')\n",
        "for chunk in doc.noun_chunks: \n",
        "  print(chunk)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFTwhtEgKFws",
        "outputId": "11b6ad87-2c0e-454d-df8d-ad03b93c8fde"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A noun chunk\n",
            "a phrase\n",
            "that\n",
            "a noun\n",
            "its head\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#64 page\n",
        "for token in doc: \n",
        "  if token.pos_=='NOUN':\n",
        "    chunk = ''\n",
        "    for w in token.children: \n",
        "      if w.pos_ == 'DET' or w.pos_ == 'ADJ':\n",
        "              chunk = chunk + w.text + ' ' \n",
        "      chunk = chunk + token.text  \n",
        "print(chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7i77vL3KT_g",
        "outputId": "ddc5fe75-b1c8-4610-c6ff-a431fe14a1db"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "head\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#66 page\n",
        "doc=nlp('I want a green apple.') \n",
        "print(doc[2:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIODK5z2KOwK",
        "outputId": "d8aaba7d-f4e7-4130-bbcd-5818913a6318"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a green apple\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#66 page\n",
        "doc = nlp(u'The Golden Gate Bridge is an iconic landmark in San Francisco.')\n",
        "print([doc[i] for i in range(len(doc))] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHlMgFFkK0LP",
        "outputId": "b6d0450d-b35e-46e1-bf34-01fda95b2b66"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[The, Golden, Gate, Bridge, is, an, iconic, landmark, in, San, Francisco, .]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'The Golden Gate Bridge is an iconic landmark in San Francisco.')\n",
        "span = doc[1:4]\n",
        "lem_id = doc.vocab.strings[span.text]\n",
        "print(span.)\n",
        "#print(span.merge(lemma = lem_id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL0-0sipK9kw",
        "outputId": "09c57d06-e3e9-47a4-9e1e-fa7307a77d58"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Golden Gate Bridge\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.attrs import LEMMA\n",
        "\n",
        "doc = nlp(\"I ran 3km yesterday.\")\n",
        "span = doc[2:4]\n",
        "with doc.retokenize() as retokenizer:\n",
        "   retokenizer.merge(span, attrs={LEMMA: doc.vocab.strings[span.text]})\n",
        "print(span)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcCgB2LKL9fQ",
        "outputId": "448bbd1a-9013-4d06-aa0d-adec725c341c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3km yesterday\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#68 page\n",
        "for token in doc:      \n",
        "  print(token.text, token.lemma_, token.pos_, token.dep_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD3WvpXuMpt2",
        "outputId": "c86ab671-2283-4d7a-f521-4d5fb6e21abd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I I PRON nsubj\n",
            "ran run VERB ROOT\n",
            "3km 3km NOUN npadvmod\n",
            "yesterday yesterday NOUN npadvmod\n",
            ". . PUNCT punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#69 page\n",
        "doc = nlp(u'I want a green apple.') \n",
        "for token in doc: \n",
        "  print(token.text, token.pos_, token.dep_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JrfKvVXMwEI",
        "outputId": "13fb2a33-02f1-46ec-ee9b-0439994ddc80"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I PRON nsubj\n",
            "want VERB ROOT\n",
            "a DET det\n",
            "green ADJ amod\n",
            "apple NOUN dobj\n",
            ". PUNCT punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy \n",
        "nlp = spacy.load('en_core_web_sm') \n",
        "print(nlp.meta['lang'] + '_' + nlp.meta['name']) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-FTehH6NCEX",
        "outputId": "650c39a9-4f57-4794-c017-9b5fc4e4c37e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en_core_web_sm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#71 page \n",
        "from spacy import util\n",
        "util.get_package_path('en_core_web_sm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_s6_LkG8NWji",
        "outputId": "bf312ee2-f9bf-48e4-9843-6e5a6aab646e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/usr/local/lib/python3.7/dist-packages/en_core_web_sm')"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 71 page\n",
        "print(nlp.meta['lang'] + '_' + nlp.meta['name'] + '-' + nlp. meta['version'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duffSSfvNdLK",
        "outputId": "86a066f5-baf5-415c-97bc-ac3c2b1e70bc"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en_core_web_sm-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# page 71\n",
        "nlp.meta['pipeline']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCHuTNgdNiN4",
        "outputId": "7dd092cf-97d2-41c9-9409-21666dd2710f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# page 71\n",
        "lang = 'en'\n",
        "pipeline = ['tagger', 'parser', 'ner']\n",
        "model_data_path =  '/usr/local/lib/python3.5/site-packages/en_core_web_sm/ en_core_web_sm-2.0.0'\n",
        "lang_cls = spacy.util.get_lang_class(lang)\n",
        "nlp = lang_cls()\n",
        "for name in pipeline:\n",
        "  component = nlp.create_pipe(name)\n",
        "  #nlp.add_pipe(component)\n",
        "nlp.from_disk(model_data_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "hVSqaqkJNk7J",
        "outputId": "f806488f-0f4e-4e82-c25c-82fc28c997d6"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-aa01979f3ca7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m#nlp.add_pipe(component)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(self, path, exclude, overrides)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0;31m# Convert to list here in case exclude is (default) tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0mexclude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_link_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;31m# Split to support file names like meta.json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m             \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m   2100\u001b[0m         \u001b[0mdeserializers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialize_vocab\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2101\u001b[0m         deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n\u001b[0;32m-> 2102\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2103\u001b[0m         )\n\u001b[1;32m   2104\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_components\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.from_disk\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/pathlib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1206\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         return io.open(self, mode, buffering, encoding, errors, newline,\n\u001b[0;32m-> 1208\u001b[0;31m                        opener=self._opener)\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/pathlib.py\u001b[0m in \u001b[0;36m_opener\u001b[0;34m(self, name, flags, mode)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0o666\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m         \u001b[0;31m# A stub for the opener argument to built-in open()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_raw_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0o777\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/usr/local/lib/python3.5/site-packages/en_core_web_sm/ en_core_web_sm-2.0.0/tokenizer'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#72 page\n",
        "doc = nlp(u'I need a taxi to Festy.')\n",
        "for ent in doc.ents:\n",
        "  print(ent.lemma_, ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "Nv0stNubVvgt"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_text=\"\"\"India that previously comprised only a handful of players in the e-commerce space, is now home to many biggies and giants battling out with each other to reach the top. This is thanks to the overwhelming internet and smartphone penetration coupled with the ever-increasing digital adoption across the country. These new-age innovations not only gave emerging startups a unique platform to deliver seamless shopping experiences but also provided brick and mortar stores with a level-playing field to begin their online journeys without leaving their offline legacies.\n",
        "In the wake of so many players coming together on one platform, the Indian e-commerce market is envisioned to reach USD 84 billion in 2021 from USD 24 billion in 2017. Further, with the rate at which internet penetration is increasing, we can expect more and more international retailers coming to India in addition to a large pool of new startups. This, in turn, will provide a major Philip to the organized retail market and boost its share from 12% in 2017 to 22-25% by 2021. \n",
        "Here’s a view to the e-commerce giants that are dominating India’s online shopping space:\n",
        "Amazon – One of the uncontested global leaders, Amazon started its journey as a simple online bookstore that gradually expanded its reach to provide a large suite of diversified products including media, furniture, food, and electronics, among others. And now with the launch of Amazon Prime and Amazon Music Limited, it has taken customer experience to a godly level, which will remain undefeatable for a very long time. \n",
        "\n",
        "Flipkart – Founded in 2007, Flipkart is recognized as the national leader in the Indian e-commerce market. Just like Amazon, it started operating by selling books and then entered other categories such as electronics, fashion, and lifestyle, mobile phones, etc. And now that it has been acquired by Walmart, one of the largest leading platforms of e-commerce in the US, it has also raised its bar of customer offerings in all aspects and giving huge competition to Amazon. \n",
        "\n",
        "Snapdeal – Started as a daily deals platform in 2010, Snapdeal became a full-fledged online marketplace in 2011 comprising more than 3 lac sellers across India. The platform offers over 30 million products across 800+ diverse categories from over 125,000 regional, national, and international brands and retailers. The Indian e-commerce firm follows a robust strategy to stay at the forefront of innovation and deliver seamless customer offerings to its wide customer base. It has shown great potential for recovery in recent years despite losing Freecharge and Unicommerce. \n",
        "\n",
        "ShopClues – Another renowned name in the Indian e-commerce industry, ShopClues was founded in July 2011. It’s a Gurugram based company having a current valuation of INR 1.1 billion and is backed by prominent names including Nexus Venture Partners, Tiger Global, and Helion Ventures as its major investors. Presently, the platform comprises more than 5 lac sellers selling products in nine different categories such as computers, cameras, mobiles, etc. \n",
        "\n",
        "Paytm Mall – To compete with the existing e-commerce giants, Paytm, an online payment system has also launched its online marketplace – Paytm Mall, which offers a wide array of products ranging from men and women fashion to groceries and cosmetics, electronics and home products, and many more. The unique thing about this platform is that it serves as a medium for third parties to sell their products directly through the widely-known app – Paytm. \n",
        "\n",
        "Reliance Retail – Given Reliance Jio’s disruptive venture in the Indian telecom space along with a solid market presence of Reliance, it is no wonder that Reliance will soon be foraying into retail space. As of now, it has plans to build an e-commerce space that will be established on online-to-offline market program and aim to bring local merchants on board to help them boost their sales and compete with the existing industry leaders. \n",
        "Big Basket – India’s biggest online supermarket, Big Basket provides a wide variety of imported and gourmet products through two types of delivery services – express delivery and slotted delivery. It also offers pre-cut fruits along with a long list of beverages including fresh juices, cold drinks, hot teas, etc. Moreover, it not only provides farm-fresh products but also ensures that the farmer gets better prices. \n",
        "\n",
        "Grofers – One of the leading e-commerce players in the grocery segment, Grofers started its operations in 2013 and has reached overwhelming heights in the last 5 years. Its wide range of products includes atta, milk, oil, daily need products, vegetables, dairy products, juices, beverages, among others. With its growing reach across India, it has become one of the favorite supermarkets for Indian consumers who want to shop grocery items from the comforts of their homes. \n",
        "\n",
        "Digital Mall of Asia – Going live in 2020, Digital Mall of Asia is a very unique concept coined by the founders of Yokeasia Malls. It is designed to provide an immersive digital space equipped with multiple visual and sensory elements to sellers and shoppers. It will also give retailers exclusive rights to sell a particular product category or brand in their respective cities. What makes it unique is its zero-commission model enabling retailers to pay only a fixed amount of monthly rental instead of paying commissions. With its one-of-a-kind features, DMA is expected to bring\n",
        "never-seen transformation to the current e-commerce ecosystem while addressing all the existing e-commerce worries such as counterfeiting. \"\"\"\n",
        "\n",
        "doc=nlp(article_text)\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "iT81EFnHXFix"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#73 page\n",
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "\n",
        "LABEL = 'DISTRICT' \n",
        "TRAIN_DATA = [ \n",
        "    ('We need to deliver it to Festy.',{ \n",
        "        'entities': [(25, 30, 'DISTRICT')]   \n",
        "      }), \n",
        "      ('I like red oranges', { \n",
        "          'entities': []   \n",
        "           }) \n",
        " ]\n",
        "ner = nlp.get_pipe('ner')\n",
        "ner.add_label(LABEL)\n",
        "nlp.disable_pipes('tagger') \n",
        "nlp.disable_pipes('parser')\n",
        "\n",
        "optimizer = nlp.create_optimizer()\n",
        "import random\n",
        "for i in range(25):    \n",
        "  random.shuffle(TRAIN_DATA)    \n",
        "  for text, annotations in TRAIN_DATA:\n",
        "    nlp.update([text], [annotations], sgd=optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "LX0e-U6VV-VI",
        "outputId": "0dd904d8-0778-40cd-9385-0c3010f38e62"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-110-d1241a2f0374>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_DATA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTRAIN_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, examples, _, drop, sgd, losses, component_cfg, exclude, annotates)\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         \u001b[0mvalidate_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Language.update\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m         \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_copy_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msgd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/training/example.pyx\u001b[0m in \u001b[0;36mspacy.training.example.validate_examples\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: [E978] The Language.update method takes a list of Example objects, but got: {<class 'str'>}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#74 page\n",
        "doc = nlp(u'I need a taxi to Festy.')\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZU8vIFvXleJ",
        "outputId": "c2fa50a3-7c23-4773-c321-8922e36bb31e"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Festy WORK_OF_ART\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.language import EntityRecognizer\n",
        "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
        "ner = EntityRecognizer(nlp.vocab)\n",
        "ner.from_disk('/usr/to/ner')\n",
        "nlp.add_pipe(ner)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "itzlw9EFhskP",
        "outputId": "ecfc1cb9-0482-45c9-e2da-5c45c5e1d333"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-118-d36458d256b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEntityRecognizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_sm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ner'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEntityRecognizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/usr/to/ner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'EntityRecognizer' from 'spacy.language' (/usr/local/lib/python3.7/dist-packages/spacy/language.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "#from spacy.gold import GoldParse\n",
        "from spacy.language import EntityRecognizer\n",
        "  \n",
        "nlp = spacy.load('en', entity = False, parser = False)\n",
        "  \n",
        "doc_list = []\n",
        "doc = nlp('Llamas make great pets.')\n",
        "doc_list.append(doc)\n",
        "gold_list = []\n",
        "gold_list.append(GoldParse(doc, [u'ANIMAL', u'O', u'O', u'O']))\n",
        "  \n",
        "ner = EntityRecognizer(nlp.vocab, entity_types = ['ANIMAL'])\n",
        "ner.update(doc_list, gold_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "IaV3x9CrinDY",
        "outputId": "519caf46-4f9a-4ac0-ba73-3552313d3ac6"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-121-7743eaf35f75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#from spacy.gold import GoldParse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEntityRecognizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'EntityRecognizer' from 'spacy.language' (/usr/local/lib/python3.7/dist-packages/spacy/language.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from cymem.cymem cimport Pool\n",
        "from spacy.tokens.doc cimport Doc \n",
        "from spacy.structs cimport TokenC \n",
        "from spacy.typedefs cimport hash_t"
      ],
      "metadata": {
        "id": "msY_ub6zjE52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#82 page\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u\"The firm earned $1.5 million in 2017.\")\n",
        "for token in doc:\n",
        "  print(token.text, token.pos_, spacy.explain(token.pos_)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lYpV4VdjSSE",
        "outputId": "d096464f-efa4-450e-c46b-14029e293a05"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The DET determiner\n",
            "firm NOUN noun\n",
            "earned VERB verb\n",
            "$ SYM symbol\n",
            "1.5 NUM numeral\n",
            "million NUM numeral\n",
            "in ADP adposition\n",
            "2017 NUM numeral\n",
            ". PUNCT punctuation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#83 page\n",
        "for token in doc:\n",
        "  print(token.text, token.pos_, token.tag_, spacy.explain(token.tag_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5S_zfywjmbd",
        "outputId": "178fef86-9eed-4ba9-f50f-790834b62754"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The DET DT determiner\n",
            "firm NOUN NN noun, singular or mass\n",
            "earned VERB VBD verb, past tense\n",
            "$ SYM $ symbol, currency\n",
            "1.5 NUM CD cardinal number\n",
            "million NUM CD cardinal number\n",
            "in ADP IN conjunction, subordinating or preposition\n",
            "2017 NUM CD cardinal number\n",
            ". PUNCT . punctuation mark, sentence closer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#84 page\n",
        "import spacy \n",
        "nlp = spacy.load('en_core_web_sm') \n",
        "doc = nlp(u\"The firm earned $1.5 million in 2017.\") \n",
        "phrase = '' \n",
        "for token in doc: \n",
        "  if token.tag_ == '$':\n",
        "    phrase = token.text\n",
        "    i = token.i+1\n",
        "    while doc[i].tag_ == 'CD':\n",
        "      phrase += doc[i].text + ' '\n",
        "      i += 1  \n",
        "    break \n",
        "phrase = phrase[:-1] \n",
        "print(phrase)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9T1cRB7jrD6",
        "outputId": "27f76d2d-be01-4ddf-fb70-664bcc7b9cab"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$1.5 million\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#86 page\n",
        "doc = nlp(u\"I can promise it is worth your time.\")\n",
        "for token in doc:\n",
        "  \n",
        "  print(token.text, token.pos_, token.tag_) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Gad7uLwkKVY",
        "outputId": "e48bcb04-93fb-46df-86c7-26256e5c96e4"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I PRON PRP\n",
            "can AUX MD\n",
            "promise VERB VB\n",
            "it PRON PRP\n",
            "is AUX VBZ\n",
            "worth ADJ JJ\n",
            "your PRON PRP$\n",
            "time NOUN NN\n",
            ". PUNCT .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy \n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u\"I can promise it is worth your time.\") \n",
        "sent = '' \n",
        "for i,token in enumerate(doc): \n",
        "  if token.tag_ == 'PRP' and doc[i+1].tag_ == 'MD' and doc[i+2].tag_ == 'VB':  \n",
        "    sent =  doc[i+1].text.capitalize() + ' ' + doc[i].text      \n",
        "    sent = sent + ' ' + doc[i+2:].text\n",
        "    break \n",
        "# К этому моменту предложение должно выглядеть так: \n",
        "# 'Can I promise it is worth your time.' \n",
        "# Повторная токенизация \n",
        "doc=nlp(sent) \n",
        "for i,token in enumerate(doc):  \n",
        "  if token.tag_ == 'PRP' and token.text == 'I':      \n",
        "    sent = doc[:i].text + ' you ' +  doc[i+1:].text      \n",
        "    break \n",
        "# К этому моменту предложение должно выглядеть так: \n",
        "# 'Can you promise it is worth your time.' \n",
        "doc=nlp(sent) \n",
        "for i,token in enumerate(doc):  \n",
        "  if token.tag_ == 'PRP$' and token.text == 'your':      \n",
        "    sent = doc[:i].text + ' my ' +  doc[i+1:].text      \n",
        "    break \n",
        "# К этому моменту предложение должно выглядеть так: \n",
        "# 'Can you promise it is worth my time.' \n",
        "doc=nlp(sent) \n",
        "for i,token in enumerate(doc):  \n",
        "  if token.tag_ == 'VB': \n",
        "    sent = doc[:i].text + ' really ' +  doc[i:].text       \n",
        "    break \n",
        "# К этому моменту предложение должно выглядеть так: \n",
        "# 'Can you really promise it is worth my time.' \n",
        "doc=nlp(sent) \n",
        "sent = doc[:len(doc)-1].text + '?' \n",
        "# Наконец, мы получаем: 'Can you really promise it is worth my time?' \n",
        "print(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOLvnNmAkQxD",
        "outputId": "e1e4e335-4438-4361-f49d-9edfae7fd3e8"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can you really promise it is worth my time?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#91 page\n",
        "doc = nlp(u\"I can promise it is worth your time.\")\n",
        "for token in doc:\n",
        "  print(token.text, token.pos_, token.tag_, token.dep_, spacy. explain(token.dep_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxqU0M8ElR-R",
        "outputId": "0d11c83c-1c1e-4109-b76f-bf77ea6ac004"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I PRON PRP nsubj nominal subject\n",
            "can AUX MD aux auxiliary\n",
            "promise VERB VB ROOT root\n",
            "it PRON PRP nsubj nominal subject\n",
            "is AUX VBZ ccomp clausal complement\n",
            "worth ADJ JJ acomp adjectival complement\n",
            "your PRON PRP$ poss possession modifier\n",
            "time NOUN NN npadvmod noun phrase as adverbial modifier\n",
            ". PUNCT . punct punctuation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy \n",
        "import sys\n",
        "def find_chunk(doc):  \n",
        "  chunk = ''  \n",
        "  for i,token in enumerate(doc): \n",
        "    if token.dep_ == 'dobj':   \n",
        "      shift = len([w for w in token.children])   \n",
        "      #print([w for w in token.children])\n",
        "      chunk = doc[i-shift:i+1]     \n",
        "      break  \n",
        "  return chunk\n"
      ],
      "metadata": {
        "id": "AnxcBHDPlf91"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_question_type(chunk):  \n",
        "  question_type = 'yesno'  \n",
        "  for token in chunk:\n",
        "    if token.dep_ == 'amod': \n",
        "      question_type = 'info'  \n",
        "  return question_type"
      ],
      "metadata": {
        "id": "J6xDc-y4ltpo"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_question(doc, question_type):  \n",
        "  sent = ''  \n",
        "  for i,token in enumerate(doc):    \n",
        "    if token.tag_ == 'PRP' and doc[i+1].tag_ == 'VBP':      \n",
        "      sent =  'do ' + doc[i].text      \n",
        "      sent = sent + ' ' + doc[i+1:].text      \n",
        "      break  \n",
        "  doc=nlp(sent)  \n",
        "  for i,token in enumerate(doc):    \n",
        "    if token.tag_ == 'PRP' and token.text == 'I':      \n",
        "      sent = doc[:i].text + ' you ' +  doc[i+1:].text      \n",
        "      break  \n",
        "  doc=nlp(sent)  \n",
        "  if question_type == 'info':  \n",
        "      for i,token in enumerate(doc):  \n",
        "        if token.dep_ == 'dobj':        \n",
        "          sent = 'why ' + doc[:i].text + ' one ' +  doc[i+1:].text        \n",
        "          break  \n",
        "  if question_type == 'yesno':    \n",
        "    for i,token in enumerate(doc): \n",
        "      if token.dep_ == 'dobj':\n",
        "        sent = doc[:i-1].text + ' a red ' +  doc[i:].text        \n",
        "        break  \n",
        "  doc=nlp(sent)  \n",
        "  sent = doc[0].text.capitalize() +' ' + doc[1:len(doc)-1].text + '?'  \n",
        "  return sent"
      ],
      "metadata": {
        "id": "50TgwP7Tl6ee"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "if len(sys.argv) > 1:  \n",
        "  sent = sys.argv[1]  \n",
        "  nlp = spacy.load('en_core_web_sm')  \n",
        "  doc = nlp(sent)  \n",
        "  chunk = find_chunk(doc)  \n",
        "  if str(chunk) == '':    \n",
        "    print('The sentence does not contain a direct object.')    \n",
        "    sys.exit()  \n",
        "    question_type = determine_question_type(chunk)  \n",
        "    question = generate_question(doc, question_type)  \n",
        "    print(question) \n",
        "  else:  \n",
        "    print('You did not submit a sentence!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "QiN2eZSzmzVI",
        "outputId": "8dcb34f4-f6e7-43ad-dac3-3a6defd803a7"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentence does not contain a direct object.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#104 page\n",
        "doc=nlp('I want a green apple.')\n",
        "doc.similarity(doc[2:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p92QnwEdnmAL",
        "outputId": "27a5d2af-ee7c-417f-9fcc-f4bc9114db92"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6240444557468489"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#105 page\n",
        "doc.similarity(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kis0p8WPn0wU",
        "outputId": "a20810d5-b277-47cb-fd5f-596aeb567626"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#105 page\n",
        "doc[2:5].similarity(doc[2:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4MQiodWn3do",
        "outputId": "cff0c47f-5f21-4d2a-f5b2-73826f353f72"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#105 page\n",
        "doc2=nlp('I like red oranges.')\n",
        "doc2.similarity(doc[2:5]) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSFEVDJ1n989",
        "outputId": "789d637d-178a-451e-a7ac-e4721f6f5f2d"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.22478505229070148"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#105 page\n",
        "token = doc2[3:4][0] \n",
        "print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYF-48YDoE4f",
        "outputId": "22ce9557-d216-49ab-f1eb-14df3fee9aa3"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "oranges\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#105 page\n",
        "token.similarity(doc[4:5]) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQYW1LeUoIle",
        "outputId": "22d47648-848d-4a73-ea0d-9dfc7bf1aa71"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5848100781440735"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#109 page \n",
        "import spacy \n",
        "nlp = spacy.load('en_core_web_sm') \n",
        "token = nlp(u'fruits')[0] \n",
        "doc = nlp(u'I want to buy this beautiful book at the end of the week. Sales of citrus have increased over the last year. How much do you know about this type of tree?') \n",
        "for sent in doc.sents:\n",
        "  print(sent.text)  \n",
        "  print('similarity to', token.text, 'is', token.similarity(sent),'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEyDWuk0oeui",
        "outputId": "91dcb436-ba12-4818-8198-6e9568b3b0a6"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I want to buy this beautiful book at the end of the week.\n",
            "similarity to fruits is -0.04223029315471649 \n",
            "\n",
            "Sales of citrus have increased over the last year.\n",
            "similarity to fruits is -0.027368908748030663 \n",
            "\n",
            "How much do you know about this type of tree?\n",
            "similarity to fruits is -0.07132802903652191 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  import sys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#111-112 pages\n",
        "import spacy \n",
        "nlp = spacy.load('en_core_web_sm') \n",
        "token = nlp(u'fruits')[0] \n",
        "doc = nlp(u'I want to buy this beautiful book at the end of the week. Sales of citrus have increased over the last year. How much do you know about this type of tree?') \n",
        "similarity = {} \n",
        "for i, sent in enumerate(doc.sents):\n",
        "  noun_span_list = [sent[j].text for j in range(len(sent)) if   sent[j].pos_ == 'NOUN']  \n",
        "  noun_span_str = ' '.join(noun_span_list)  \n",
        "  noun_span_doc = nlp(noun_span_str)  \n",
        "  similarity.update({i:token.similarity(noun_span_doc)}) \n",
        "print(similarity)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HY0btWuSpGnF",
        "outputId": "5babc6fc-a906-4528-801d-4e9b69566cdd"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 0.0726100970506393, 1: 0.2306078439472607, 2: 0.31927317721232235}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy \n",
        "nlp = spacy.load('en_core_web_sm') \n",
        "# Первый пример текста \n",
        "doc1 = nlp(u'Google Search, often referred to as simply Google, is the most used search engine nowadays. It handles a huge number of searches each day.') \n",
        "# Второй пример текста \n",
        "doc2 = nlp(u'Microsoft Windows is a family of proprietary operating systems developed and sold by Microsoft. The company also produces a wide range of other software for desktops and servers.') \n",
        "# Третий пример текста \n",
        "doc3 = nlp(u\"Titicaca is a large, deep, mountain lake in the Andes. It is known as the highest navigable lake in the world.\") \n",
        "docs = [doc1,doc2,doc3]\n",
        "spans = {} \n",
        "for j,doc in enumerate(docs):  \n",
        "  named_entity_span = [doc[i].text for i in range(len(doc)) if  doc[i].ent_type != 0]  \n",
        "  print(named_entity_span)  \n",
        "  named_entity_span = ' '.join(named_entity_span)  \n",
        "  named_entity_span = nlp(named_entity_span)  \n",
        "  spans.update({j:named_entity_span})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41NSe2K0p6Bt",
        "outputId": "7bf9cd29-7747-4b71-f6d0-527a41cf7304"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Google', 'Search', 'Google', 'each', 'day']\n",
            "['Microsoft', 'Windows', 'Microsoft']\n",
            "['Andes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('doc1 is similar to doc2:',spans[0].similarity(spans[1])) \n",
        "print('doc1 is similar to doc3:',spans[0].similarity(spans[2])) \n",
        "print('doc2 is similar to doc3:',spans[1].similarity(spans[2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-kslquCqhCN",
        "outputId": "5aefc0bf-0556-43a6-8424-6b50a580ffa1"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doc1 is similar to doc2: 0.7054618244550298\n",
            "doc1 is similar to doc3: 0.28571590894622595\n",
            "doc2 is similar to doc3: 0.39527563901549617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YFl9UsJAqjxA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}